{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from os.path import join\n",
    "import sys\n",
    "base_path = os.path.abspath(os.path.join('..'))\n",
    "if base_path not in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "from pprint import pprint\n",
    "    \n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.stats import entropy, normaltest, mode\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import seaborn\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize']\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import itertools\n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "\n",
    "from time import time, strftime\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"matplotlib\")\n",
    "\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.analysis import *\n",
    "from helpers.processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plotly_prediction(qualtrics_id):\n",
    "    if ('bar' in qualtrics_id): return 1\n",
    "    if ('line' in qualtrics_id): return 2\n",
    "    if ('scatter' in qualtrics_id): return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'results_dir': '../results',\n",
    "    'intermediate_dir': '../intermediate_results',\n",
    "    'mode': 'two',\n",
    "    'model': 'dict', #  'model',\n",
    "    'num_simulated_votes': None,\n",
    "    'num_bootstraps': 100000,\n",
    "    'num_mturk_splits': 100000,\n",
    "    'load_existing_simulations': True,\n",
    "    'load_accuracy': True,\n",
    "    'mturk_accuracy_method': 'real'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_dir = config['results_dir']\n",
    "if not os.path.exists(base_results_dir):\n",
    "    os.mkdir(base_results_dir)\n",
    "    \n",
    "base_intermediate_dir = config['intermediate_dir']\n",
    "if not os.path.exists(base_intermediate_dir):\n",
    "    os.mkdir(base_intermediate_dir)\n",
    "    \n",
    "results_dir = join(base_results_dir, strftime('%Y-%m-%d'))\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_data_directory = '../experiment_data'\n",
    "\n",
    "if config['mode'] == 'two':\n",
    "    experiment_raw_data = 'v3_two_type_raw_data.csv'\n",
    "    ground_truth_fids_df = pd.read_csv(join(experiment_data_directory, 'ground_truth_fids_66.csv'))\n",
    "if config['mode'] == 'three':\n",
    "    experiment_raw_data = 'v3_three_type_raw_data.csv'\n",
    "    ground_truth_fids_df = pd.read_csv(join(experiment_data_directory, 'ground_truth_fids_99.csv'))\n",
    "\n",
    "outcome_label_to_value = { 'bar': 1, 'line': 2, 'scatter': 3 }\n",
    "qualtrics_id_to_fid = {}\n",
    "for outcome_label, outcome_value in outcome_label_to_value.items():\n",
    "    for i, row in ground_truth_fids_df[ground_truth_fids_df.plotly == outcome_value].reset_index().iterrows():\n",
    "        qualtrics_id_to_fid['{}_{}'.format(outcome_label, i + 1)] = row.fid\n",
    "        \n",
    "df = pd.read_csv(join(experiment_data_directory, experiment_raw_data))\n",
    "df = df.iloc[2:, :]\n",
    "df = df[df['how_easy'].notna()]\n",
    "type_columns = natural_sort([ c for c in df.columns if ('_type' in c)])\n",
    "df = df[type_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ground_truth_predictions_dict = {}\n",
    "if config['model'] == 'model':\n",
    "    top_performing_model_dir = ''\n",
    "    # top_performing_model_file_name = 'clf__model-rf__dataset-dataset__featureset-names__outcome-all_one_trace_type__task-two__perclass-296203__acc-0.938055.pkl'\n",
    "    top_performing_model_file_name = 'clf__model-rf__dataset-dataset__featureset-names__outcome-all_one_trace_type__task-three__perclass-296202__acc-0.878786.pkl'\n",
    "    top_performing_model = joblib.load(join(base_results_dir, '', top_performing_model_file_name))\n",
    "    clf = top_performing_model\n",
    "    \n",
    "    ground_truth_fids = ground_truth_fids_df.fid\n",
    "    ground_truth_features_file_name = '{}_ground_truth.csv'.format(features_df_file_name.split('.csv')[0])\n",
    "    \n",
    "    ground_truth_features_df = pd.read_csv(join(features_directory, ground_truth_features_file_name))\n",
    "    ground_truth_features_df = ground_truth_features_df[ground_truth_features_df.fid.isin(ground_truth_fids)]\n",
    "\n",
    "    feature_set_names = [ c for c in feature_set_names if c in ground_truth_features_df.columns ]\n",
    "    feature_set_indices = [ ground_truth_features_df.columns.get_loc(c) for c in feature_set_names if c in ground_truth_features_df.columns]\n",
    "\n",
    "    ground_truth_features_df.sort_values(by=['fid'])\n",
    "    sorted_fids = ground_truth_features_df.fid\n",
    "    X_ground_truth = ground_truth_features_df.drop(['fid'], axis=1, inplace=False, errors='ignore')\n",
    "    X_ground_truth = X_ground_truth[feature_set_names]\n",
    "    \n",
    "    # With loaded model\n",
    "    ground_truth_predictions = [ outcome_label_to_value[x] for x in clf.predict(X_ground_truth)]\n",
    "    ground_truth_predictions_dict = dict((x, y) for (x, y) in zip(sorted_fids, ground_truth_predictions))\n",
    "elif config['model'] == 'dict':\n",
    "    if config['mode'] == 'two':\n",
    "        ground_truth_predictions_dict_name = 'task_1_nn_ground_truth_2018-05-22_one-per-user.json'\n",
    "    elif config['mode'] == 'three':\n",
    "        ground_truth_predictions_dict_name = 'task_2_nn_ground_truth_2018-05-22_one-per-user.json'\n",
    "    ground_truth_predictions_dict = dict((k, outcome_label_to_value[v]) for (k, v) in json.load(open(join(experiment_data_directory, ground_truth_predictions_dict_name))).items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_data.data2vis_predictions import data2vis_predictions_raw\n",
    "from experiment_data.deepeye_predictions import deepeye_predictions_raw\n",
    "from experiment_data.showme_predictions import showme_predictions_raw\n",
    "from experiment_data.compassql_predictions import compassql_predictions_raw\n",
    "\n",
    "\n",
    "data2vis_predictions = {}\n",
    "for k, v in data2vis_predictions_raw.items():\n",
    "    qualtrics_id = k.rsplit('_type')[0]\n",
    "    if qualtrics_id in qualtrics_id_to_fid:\n",
    "        fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "        data2vis_predictions[fid] = v\n",
    "\n",
    "deepeye_predictions = {}\n",
    "for k, v in deepeye_predictions_raw.items():\n",
    "    qualtrics_id = k.rsplit('_type')[0]\n",
    "    if qualtrics_id in qualtrics_id_to_fid:\n",
    "        fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "        deepeye_predictions[fid] = v\n",
    "\n",
    "showme_predictions = {}\n",
    "for k, v in showme_predictions_raw.items():\n",
    "    qualtrics_id = k.rsplit('_type')[0]\n",
    "    if qualtrics_id in qualtrics_id_to_fid:\n",
    "        fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "        showme_predictions[fid] = v\n",
    "        \n",
    "compassql_predictions = {}\n",
    "for k, v in compassql_predictions_raw.items():\n",
    "    qualtrics_id = k.rsplit('_type')[0]\n",
    "    if qualtrics_id in qualtrics_id_to_fid:\n",
    "        fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "        compassql_predictions[fid] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vote_results_without_na = dict([ (c, df[c].dropna().astype(int)) for c in df ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_votes_per_chart = [ len(v) for v in vote_results_without_na.values() ]\n",
    "np.mean(num_votes_per_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consensus-Adjusted Recommendation Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_votes = False\n",
    "if load_votes:\n",
    "    vote_results_without_na = dict([ (c, df[c].dropna().astype(int)) for c in df ])\n",
    "    bootstrapped_votes_file_name = 'simulated_votes_{}_{}-type.pkl'.format(config['num_simulated_votes'], config['mode'])\n",
    "    all_sample_modes_file_name = 'simulated_modes_{}_{}-type.pkl'.format(config['num_simulated_votes'], config['mode'])\n",
    "\n",
    "    bootstrapped_votes = pickle.load(open(bootstrapped_votes_file_name, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_probas = True\n",
    "if load_probas:\n",
    "    real_probas = pickle.load(\n",
    "        open(join(base_intermediate_dir, 'real_probas_{}_{}-type.pkl'.format(config['num_simulated_votes'], config['mode'])), 'rb'))\n",
    "    all_sample_probas = pickle.load(\n",
    "        open(join(base_intermediate_dir, 'all_samples_probas_{}_{}-type.pkl'.format(config['num_simulated_votes'], config['mode'])), 'rb'))\n",
    "else:\n",
    "    def get_probas(votes):\n",
    "        counts = Counter()\n",
    "        for v in votes:\n",
    "            counts[v] += 1\n",
    "        probas = {}\n",
    "        num_votes = len(votes)\n",
    "        for k, v in counts.items():\n",
    "            probas[k] = v / num_votes\n",
    "        return probas\n",
    "\n",
    "    real_probas = {}\n",
    "    for c, votes in vote_results_without_na.items():\n",
    "        real_probas[c] = get_probas(votes)\n",
    "\n",
    "    num_bootstraps = 100000\n",
    "    all_sample_probas = []\n",
    "    for bootstrapped_voteset in bootstrapped_votes[:num_bootstraps]:\n",
    "        bootstrapped_probas = {}\n",
    "        for c, votes in bootstrapped_voteset.items():\n",
    "            bootstrapped_probas[c] = get_probas(votes)\n",
    "        all_sample_probas.append(bootstrapped_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_probas = False\n",
    "if save_probas:\n",
    "    pickle.dump(real_probas, open(\n",
    "        join(base_intermediate_dir, 'real_probas_{}_{}-type.pkl').format(config['num_simulated_votes'], config['mode']), 'wb'))\n",
    "    pickle.dump(all_sample_probas, open(\n",
    "        join(base_intermediate_dir, 'all_samples_probas_{}_{}-type.pkl').format(config['num_simulated_votes'], config['mode']), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(prediction, probas, classifier='deepeye'):\n",
    "    scores = {}\n",
    "    max_proba = max(probas.values())\n",
    "    for k, v in probas.items():\n",
    "        scores[k] = v / max_proba\n",
    "        \n",
    "    min_score = min(scores.values())\n",
    "    random_score = random.choice([ s for s in scores.values() ])\n",
    "    \n",
    "    if prediction == 'error':\n",
    "        return random_score\n",
    "    \n",
    "    if prediction in ['tiacle', 'none']:\n",
    "        return random_score\n",
    "    \n",
    "    if classifier == 'deepeye':\n",
    "        if prediction == 3:\n",
    "            prediction = 2\n",
    "    \n",
    "    # Normalized by the max\n",
    "    return scores.get(prediction, min_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_score(all_sample_probas):\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for c, probas in sample_probas.items():\n",
    "            random_score = random.choice([v for v in probas.values()]) / max(probas.values())\n",
    "            total_score += random_score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_score(all_sample_probas):\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for c, probas in sample_probas.items():\n",
    "            min_score = min(probas.values()) / max(probas.values())\n",
    "            total_score += min_score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data2vis_scores(all_sample_probas):\n",
    "    print('Calculating Data2Vis Scores')\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for k, probas in sample_probas.items():\n",
    "            qualtrics_id = k.rsplit('_type')[0]\n",
    "            fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "            prediction = data2vis_predictions[fid]\n",
    "            score = get_score(prediction, probas)\n",
    "            total_score += score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deepeye_scores(all_sample_probas):\n",
    "    print('Calculating DeepEye Scores')\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for k, probas in sample_probas.items():\n",
    "            qualtrics_id = k.rsplit('_type')[0]\n",
    "            fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "            prediction = deepeye_predictions[fid]\n",
    "            score = get_score(prediction, probas, classifier='deepeye')\n",
    "            total_score += score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_showme_scores(all_sample_probas):\n",
    "    print('Calculating ShowMe Scores')\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for k, probas in sample_probas.items():\n",
    "            qualtrics_id = k.rsplit('_type')[0]\n",
    "            fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "            prediction = showme_predictions[fid]\n",
    "            score = get_score(prediction, probas)\n",
    "            total_score += score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compassql_scores(all_sample_probas):\n",
    "    print('Calculating CompassQL Scores')\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for k, probas in sample_probas.items():\n",
    "            qualtrics_id = k.rsplit('_type')[0]\n",
    "            fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "            prediction = compassql_predictions[fid]\n",
    "            score = get_score(prediction, probas)\n",
    "            total_score += score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vizml_scores(all_sample_probas):\n",
    "    print('Calculating VizML Scores')\n",
    "    vizml_scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for k, probas in sample_probas.items():\n",
    "            qualtrics_id = k.rsplit('_type')[0]\n",
    "            fid = qualtrics_id_to_fid[qualtrics_id]\n",
    "            prediction = ground_truth_predictions_dict[fid]\n",
    "            score = get_score(prediction, probas)\n",
    "            total_score += score\n",
    "        vizml_scores.append(total_score)\n",
    "    return vizml_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plotly_scores(all_sample_probas):\n",
    "    print('Calculating Plot.ly Accuracies')\n",
    "    scores = []\n",
    "    for sample_probas in all_sample_probas:\n",
    "        total_score = 0\n",
    "        for k, probas in sample_probas.items():\n",
    "            prediction = get_plotly_prediction(k)\n",
    "            score = get_score(prediction, probas)\n",
    "            total_score += score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mturk_scores(vote_results_without_na, real_probas, num_splits=1000, split_size=1):\n",
    "    scores = []\n",
    "    print('Split size:', split_size)\n",
    "\n",
    "    for i in range(0, num_splits):\n",
    "        if (i % 1000 == 0): print(i)\n",
    "            \n",
    "        total_score = 0\n",
    "        \n",
    "        for c, votes in vote_results_without_na.items():\n",
    "            # probas = real_probas[c]\n",
    "     \n",
    "            num_votes = len(votes)\n",
    "            if split_size == 1: real_split_size = 1\n",
    "            else: real_split_size = math.ceil(split_size * num_votes)\n",
    "\n",
    "            votes = list(votes)\n",
    "            random.shuffle(votes)\n",
    "\n",
    "            consensus_slice = votes[:real_split_size]\n",
    "            test_slice = votes[real_split_size:]\n",
    "            consensus_mode = mode(consensus_slice).mode[0]\n",
    "            # test_mode = mode(test_slice).mode[0]\n",
    "\n",
    "            probas = get_probas(test_slice)\n",
    "            score = get_score(consensus_mode, probas)\n",
    "            total_score += score\n",
    "        scores.append(total_score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracies = {\n",
    "    # 'plotly': get_plotly_scores(all_sample_probas),\n",
    "    # 'vizml': get_vizml_scores(all_sample_probas),\n",
    "    'compassql': get_compassql_scores(all_sample_probas),\n",
    "    # 'data2viz': get_data2vis_scores(all_sample_probas),\n",
    "    # 'deepeye': get_deepeye_scores(all_sample_probas),\n",
    "    # 'minimum': get_min_score(all_sample_probas),\n",
    "    # 'random': get_random_score(all_sample_probas),\n",
    "    'showme': get_showme_scores(all_sample_probas),\n",
    "    # 'mturk_1': get_mturk_scores(vote_results_without_na, real_probas, num_splits=config['num_mturk_splits'], split_size=1),\n",
    "    # 'mturk_50': get_mturk_scores(vote_results_without_na, real_probas, num_splits=config['num_mturk_splits'], split_size=0.5),\n",
    "}\n",
    "\n",
    "if config['mode'] == 'two': total = 66\n",
    "if config['mode'] == 'three': total = 99\n",
    "normalized_accuracies = {}\n",
    "for k, v in accuracies.items():\n",
    "    normalized_accuracies[k] = np.array(v) / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Accurracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(normalized_accuracies, open(\n",
    "    join(base_intermediate_dir, 'cars_benchmark_{}_{}_both_random-type.pkl').format(config['num_simulated_votes'], config['mode']), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['load_accuracy']:\n",
    "    normalized_accuracies = pickle.load(\n",
    "        open(join(base_intermediate_dir, 'cars_benchmark_{}_{}_both_random-type.pkl').format(config['num_simulated_votes'], config['mode']), 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart of Accuracies with CI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_int = {}\n",
    "for classifier, accuracies in normalized_accuracies.items():\n",
    "    raw_scores = get_conf_int(accuracies, 0.95)\n",
    "    final_scores = {}\n",
    "    for k, v in raw_scores.items():\n",
    "        final_scores[k] = v * 100\n",
    "    conf_int[classifier] = final_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in conf_int.items():\n",
    "    print(k, v['mean'], v['upper'] - v['mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(width, height))\n",
    "plt.figure(figsize=(10, 7.5))\n",
    "# sns.set_style(\"whitegrid\")\n",
    "\n",
    "legend = (config['mode'] == 'three')\n",
    "\n",
    "colors_dict = {\n",
    "    'green': '#009E73',\n",
    "    'grey': '#A8A496',\n",
    "    'blue': '#0072B2',\n",
    "    'orange': '#D55E00',\n",
    "    'pink': '#CC79A7'\n",
    "}\n",
    "\n",
    "accuracies_df = pd.DataFrame({\n",
    "    'estimators': ['Random', 'Plotly', 'VizML', 'Data2Viz', 'DeepEye', 'MTurk', 'Show Me', 'CompassQL'],  # 'Minimum', , 'Group\\nMTurk\\n(50%)'],\n",
    "    'accuracies': [\n",
    "        # conf_int['minimum']['mean'],\n",
    "        conf_int['random']['mean'],\n",
    "        conf_int['plotly']['mean'],\n",
    "        conf_int['vizml']['mean'],\n",
    "        conf_int['data2viz']['mean'],\n",
    "        conf_int['deepeye']['mean'],\n",
    "        conf_int['mturk_1']['mean'],\n",
    "        conf_int['showme']['mean'],\n",
    "        conf_int['compassql']['mean'],\n",
    "        #conf_int['mturk_10']['mean'],\n",
    "        #conf_int['mturk_50']['mean']\n",
    "    ],\n",
    "    'errors': [ \n",
    "        # conf_int['minimum']['error'],\n",
    "        conf_int['random']['error'],\n",
    "        conf_int['plotly']['error'],\n",
    "        conf_int['vizml']['error'],\n",
    "        conf_int['data2viz']['error'],\n",
    "        conf_int['deepeye']['error'],\n",
    "        conf_int['mturk_1']['error'],\n",
    "        conf_int['showme']['error'],\n",
    "        conf_int['compassql']['error'],\n",
    "    ],\n",
    "    'colors': [ \n",
    "        colors_dict['grey'],\n",
    "        colors_dict['blue'],\n",
    "        colors_dict['green'],\n",
    "        colors_dict['green'],\n",
    "        colors_dict['green'],\n",
    "        colors_dict['blue'],\n",
    "        colors_dict['orange'],\n",
    "        colors_dict['orange']\n",
    "    \n",
    "})\n",
    "\n",
    "plt.rcParams['font.family'] = 'Helvetica Neue LT Com'\n",
    "\n",
    "accuracies_df['accuracies'] = accuracies_df['accuracies']\n",
    "accuracies_df['errors'] = accuracies_df['errors']\n",
    "\n",
    "accuracies_df.sort_values(['accuracies'], ascending=True, inplace=True)\n",
    "\n",
    "plt.rcParams['errorbar.capsize']=10\n",
    "plt.rcParams['lines.markeredgewidth']=1.5\n",
    "\n",
    "ax = accuracies_df.plot(\n",
    "    kind='bar',\n",
    "    x='estimators',\n",
    "    y='accuracies',\n",
    "    yerr='errors',\n",
    "    color=accuracies_df['colors'],\n",
    "    figsize=(width, height),\n",
    "    width=0.8,\n",
    "    linewidth=1,\n",
    "    legend=legend,\n",
    "    grid=False,\n",
    "    alpha=1,\n",
    "    rot=0,\n",
    ")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        np.round(p.get_height(),decimals=1),\n",
    "        (p.get_x()+p.get_width()/2., 0.005),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        xytext=(0, 10),\n",
    "        textcoords='offset points',\n",
    "        bbox=dict(boxstyle='Square', fc='white', lw=0, alpha=0.8)\n",
    "    )\n",
    "\n",
    "ax.yaxis.grid(which='major')\n",
    "ax.yaxis.set_ticks(np.arange(0, 101, 10))\n",
    "ax.yaxis.set_label_coords(-0.08, 0.5)\n",
    "ax.set_ylim([0, 100])\n",
    "\n",
    "print(conf_int['minimum']['mean'])\n",
    "ax.axhline(\n",
    "    y=conf_int['minimum']['mean'],\n",
    "    color='gray',\n",
    "    linestyle='dashed'\n",
    ")\n",
    "\n",
    "ax.axhline(\n",
    "    y=np.max(accuracies_df['accuracies']),\n",
    "    color='gray',\n",
    "    linestyle='dotted'\n",
    ")\n",
    "\n",
    "baseline = mpatches.Patch(color=colors_dict['grey'], label='Baseline')\n",
    "machine = mpatches.Patch(color=colors_dict['blue'], label='Human')\n",
    "single_human = mpatches.Patch(color=colors_dict['green'], label='ML-based')\n",
    "rule = mpatches.Patch(color=colors_dict['orange'], label='Rule-based')\n",
    "\n",
    "if legend:\n",
    "    pylab.legend(handles=[baseline, machine, rule, single_human], ncol=4, loc=9, bbox_to_anchor=(0.5, -0.1))\n",
    "\n",
    "rotate_x_labels = False\n",
    "offset_y_x_labels = False\n",
    "\n",
    "for tick in ax.xaxis.get_major_ticks()[1::2]:\n",
    "    tick.set_pad(15)\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('CARS')\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(results_dir, \"ground_truth_accuracy_bar_charts_{}_random_for_error_random_for_other_with_rules.png\".format(config['mode'])), format=\"png\", bbox_inches='tight')\n",
    "plt.savefig(join(results_dir, \"ground_truth_accuracy_bar_charts_{}_random_for_error_random_for_other_with_rules.svg\".format(config['mode'])), format=\"svg\", bbox_inches='tight')\n",
    "plt.savefig(join(results_dir, \"ground_truth_accuracy_bar_charts_{}_random_for_error_random_for_other_with_rules.pdf\".format(config['mode'])), format=\"pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(width, height))\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['font.family'] = 'Helvetica Neue LT Com'\n",
    "plt.rcParams['font.weight'] = 'light'\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "legend = (config['mode'] == 'three')\n",
    "\n",
    "colors_dict = {\n",
    "    'green': '#009E73',\n",
    "    'grey': '#A8A496',\n",
    "    'blue': '#0072B2',\n",
    "    'orange': '#D55E00',\n",
    "    'pink': '#CC79A7'\n",
    "}\n",
    "\n",
    "accuracies_df = pd.DataFrame({\n",
    "    'estimators': ['Random', 'Plotly', 'VizML', 'Data2Viz', 'DeepEye', 'MTurk', 'Show Me', 'CompassQL'],  # 'Minimum', , 'Group\\nMTurk\\n(50%)'],\n",
    "    'accuracies': [ conf_int[predictor]['mean'] for predictor in [ 'random', 'plotly', 'vizml', 'data2viz', 'deepeye', 'mturk_1', 'showme', 'compassql']],\n",
    "    'errors': [ conf_int[predictor]['error'] for predictor in [ 'random', 'plotly', 'vizml', 'data2viz', 'deepeye', 'mturk_1', 'showme', 'compassql']],\n",
    "    'colors': [ \n",
    "        # colors_dict['grey'],\n",
    "        colors_dict['grey'],\n",
    "        colors_dict['blue'],\n",
    "        colors_dict['green'],\n",
    "        colors_dict['green'],\n",
    "        colors_dict['green'],\n",
    "        colors_dict['blue'],\n",
    "        colors_dict['orange'],\n",
    "        colors_dict['orange']\n",
    "    ]\n",
    "})\n",
    "\n",
    "accuracies_df['accuracies'] = accuracies_df['accuracies']\n",
    "accuracies_df['errors'] = accuracies_df['errors']\n",
    "\n",
    "accuracies_df.sort_values(['accuracies'], ascending=True, inplace=True)\n",
    "\n",
    "plt.rcParams['errorbar.capsize']=5\n",
    "plt.rcParams['lines.markeredgewidth']=1\n",
    "\n",
    "ax = accuracies_df.plot(\n",
    "    kind='barh',\n",
    "    x='estimators',\n",
    "    y='accuracies',\n",
    "    xerr='errors',\n",
    "    color=accuracies_df['colors'],\n",
    "    # edgecolor=accuracies_df['edgecolors'],\n",
    "    figsize=(width, height + 0.5),\n",
    "    width=0.85,\n",
    "    linewidth=1,\n",
    "    legend=legend,\n",
    "    grid=False,\n",
    "    alpha=1,\n",
    "    rot=0,\n",
    ")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(\n",
    "        np.round(p.get_width(),decimals=1),\n",
    "        (8, p.get_y() - p.get_height() / 3.5),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        xytext=(0, 12),\n",
    "        textcoords='offset points',\n",
    "        bbox=dict(boxstyle='Square', fc='white', lw=0, alpha=0.8)\n",
    "    )\n",
    "\n",
    "ax.xaxis.grid(which='major')\n",
    "ax.xaxis.set_ticks(np.arange(0, 101, 10))\n",
    "ax.set_xlim([0, 100])\n",
    "\n",
    "# Baselines\n",
    "ax.axvline(\n",
    "    x=conf_int['minimum']['mean'],\n",
    "    color='gray',\n",
    "    linestyle='dashed'\n",
    ") \n",
    "\n",
    "ax.axvline(\n",
    "    x=np.max(accuracies_df['accuracies']),\n",
    "    color='gray',\n",
    "    linestyle='dotted'\n",
    ")\n",
    "\n",
    "baseline = mpatches.Patch(color=colors_dict['grey'], label='Baseline')\n",
    "machine = mpatches.Patch(color=colors_dict['blue'], label='Human')\n",
    "single_human = mpatches.Patch(color=colors_dict['green'], label='ML-based')\n",
    "rule = mpatches.Patch(color=colors_dict['orange'], label='Rule-based')\n",
    "\n",
    "if legend:\n",
    "    predictor_legend = pylab.legend(handles=[baseline, machine, rule, single_human], ncol=2, loc=9, bbox_to_anchor=(0.5, -0.1))\n",
    "    predictor_legend.set_title('Predictor Type')\n",
    "\n",
    "plt.xlabel('Consensus-Adjusted Recommendation Score') \n",
    "plt.ylabel('Predictor')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(join(results_dir, \"ground_truth_accuracy_bar_charts_{}_random_for_error_random_for_other_with_rules_horizontal.png\".format(config['mode'])), format=\"png\", bbox_inches='tight')\n",
    "plt.savefig(join(results_dir, \"ground_truth_accuracy_bar_charts_{}_random_for_error_random_for_other_with_rules_horizontal.svg\".format(config['mode'])), format=\"svg\", bbox_inches='tight')\n",
    "plt.savefig(join(results_dir, \"ground_truth_accuracy_bar_charts_{}_random_for_error_random_for_other_with_rules_horizontal.pdf\".format(config['mode'])), format=\"pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gini(x):\n",
    "    # (Warning: This is a concise implementation, but it is O(n**2)\n",
    "    # in time and memory, where n = len(x).  *Don't* pass in huge\n",
    "    # samples!)\n",
    "\n",
    "    # Mean absolute difference\n",
    "    mad = np.abs(np.subtract.outer(x, x)).mean()\n",
    "    # Relative mean absolute difference\n",
    "    rmad = mad/np.mean(x)\n",
    "    # Gini coefficient\n",
    "    g = 0.5 * rmad\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_coeffs = []\n",
    "name_to_gini = {}\n",
    "for k, probas_dict in real_probas.items():\n",
    "    probas = [ p for p in probas_dict.values() ]\n",
    "    gini = get_gini(probas)\n",
    "    gini_coeffs.append(gini)\n",
    "    name_to_gini[k] = gini\n",
    "\n",
    "fig = plt.figure(figsize=(width, height))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "upper_limit = 1/2\n",
    "if config['mode'] == 'three':\n",
    "    upper_limit = 2/3\n",
    "    \n",
    "ax = pd.Series(gini_coeffs).hist(\n",
    "    color=colors_dict['blue'],\n",
    "    alpha=1,\n",
    "    edgecolor='white',\n",
    "    bins=np.linspace(0, upper_limit, 11),\n",
    "    figsize=(width, height)\n",
    ")\n",
    "\n",
    "\n",
    "ax.set_xlim([0, upper_limit])\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.tight_layout()\n",
    "plt.xlabel('Gini Coefficient')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.savefig(join(results_dir, \"gini_coeff_{}.svg\".format(config['mode'])), format=\"svg\", bbox_inches='tight')\n",
    "plt.savefig(join(results_dir, \"gini_coeff_{}.pdf\".format(config['mode'])), format=\"pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
