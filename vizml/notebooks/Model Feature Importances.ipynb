{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "from copy import deepcopy\n",
    "from os.path import join\n",
    "import sys\n",
    "base_path = os.path.abspath(os.path.join('..'))\n",
    "if base_path not in sys.path:\n",
    "    sys.path.append(base_path)\n",
    "    \n",
    "import math\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from scipy.stats import entropy, normaltest, mode\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import seaborn\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize']\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import itertools\n",
    "from itertools import compress\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import time, strftime\n",
    "\n",
    "from pprint import pprint\n",
    "pd.options.display.max_rows = 999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.analysis import *\n",
    "from helpers.processing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_outcomes_df(outcomes_df, outcome_variable_name, outcomes, id_field='fid'):\n",
    "    print('Subsetting outcomes')\n",
    "    outcomes_df[outcome_variable_name].fillna(value=False, inplace=True)\n",
    "    outcomes_df_subset = outcomes_df[outcomes_df[outcome_variable_name].isin(outcomes)][[id_field, outcome_variable_name]]\n",
    "    return outcomes_df_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import capwords\n",
    "def format_feature_name(n):\n",
    "    name = n.replace('_', ' ')\n",
    "    agg = ''\n",
    "    if '-agg-' in name:\n",
    "        name, agg = name.split('-agg-')\n",
    "        return capwords(name), agg\n",
    "    else:\n",
    "        return name.title(), agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_feature_importance_outcomes(statistic, columns, p=[], returned_fields=['statistic', 'statistic_norm', 'feature_name_formatted']):\n",
    "    formatted_feature_names, feature_aggregations = [], []\n",
    "    for x in pd.Series(X.columns).apply(format_feature_name):\n",
    "        formatted_feature_names.append(x[0])\n",
    "        feature_aggregations.append(x[1])\n",
    "    \n",
    "    results_df = pd.DataFrame({\n",
    "        'feature_name': X.columns,\n",
    "        'feature_name_formatted': formatted_feature_names,\n",
    "        'aggregations': feature_aggregations,\n",
    "        'statistic': statistic,\n",
    "        'statistic_norm': statistic / np.nanmax(statistic),\n",
    "    })\n",
    "    \n",
    "    if p: results_df['p'] = p\n",
    "    \n",
    "    results_df.sort_values(['statistic'], ascending=False, inplace=True)\n",
    "    results_df.reset_index(inplace=True)\n",
    "\n",
    "    display(HTML(results_df[returned_fields].iloc[:, :].to_html(index=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_dir = '../results'\n",
    "features_directory = '../features/processed'\n",
    "\n",
    "model_dir = '../models'\n",
    "model_file_name = 'clf__model-rf__dataset-dataset__featureset-names__outcome-all_one_trace_type__task-two__perclass-296203__acc-0.938055.pkl'\n",
    "model='rf'\n",
    "\n",
    "dataset = 'dataset'\n",
    "feature_set = 'names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'features_directory': '../features/processed',\n",
    "    'features_df_file_name': 'features_aggregate_single_pairwise.csv',\n",
    "    'outcomes_df_file_name': 'chart_outcomes.csv',\n",
    "    'outcome_variable_name':  'has_single_src',  # 'trace_type', # 'is_ysrc',  # 'has_single_src',\n",
    "    'prediction_task': 'two',\n",
    "    'dataset': 'dataset',\n",
    "    'nrows': None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results_dir = '../results'\n",
    "if not os.path.exists(base_results_dir):\n",
    "    os.mkdir(base_results_dir)\n",
    "    \n",
    "results_dir = join(base_results_dir, strftime('%Y-%m-%d'))\n",
    "if not os.path.exists(results_dir):\n",
    "    os.mkdir(results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(\n",
    "    join(config['features_directory'], config['features_df_file_name']),\n",
    "    nrows=config['nrows']\n",
    ")\n",
    "outcomes_df = pd.read_csv(\n",
    "    join(config['features_directory'], config['outcomes_df_file_name']),\n",
    "    nrows=config['nrows']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset'] == 'dataset':\n",
    "    config['id_field'] = 'fid'\n",
    "else:\n",
    "    config['id_field'] = 'field_id'\n",
    " \n",
    "dataset_prediction_task_to_outcomes = {\n",
    "    'all_one_trace_type': {\n",
    "        'two': ['line', 'bar'],\n",
    "        'three': ['line', 'scatter', 'bar'],\n",
    "        'six': ['line', 'scatter', 'bar', 'box', 'histogram', 'pie'],\n",
    "    },\n",
    "    'has_single_src': {\n",
    "        'two': [ True, False ]\n",
    "    }\n",
    "}\n",
    "\n",
    "field_prediction_task_to_outcomes = {\n",
    "    'trace_type': {\n",
    "        'two': ['line', 'bar'],\n",
    "        'three': ['line', 'scatter', 'bar'],\n",
    "        'six': ['line', 'scatter', 'bar', 'box', 'histogram', 'heatmap'],\n",
    "    },\n",
    "    'is_xsrc': {\n",
    "        'two': [ True, False ]\n",
    "    },\n",
    "    'is_ysrc': {\n",
    "        'two': [ True, False ]\n",
    "    },\n",
    "    'is_x_or_y': {\n",
    "        'two': [ 'x', 'y' ]\n",
    "    },\n",
    "    'is_single_src': {\n",
    "        'two': [ True, False ]\n",
    "    }\n",
    "}\n",
    "if config['dataset'] == 'dataset':\n",
    "    prediction_task_to_outcomes = dataset_prediction_task_to_outcomes\n",
    "if config['dataset'] == 'field':\n",
    "    prediction_task_to_outcomes = field_prediction_task_to_outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['dataset'] == 'field':\n",
    "    def is_x_or_y(is_xsrc, is_ysrc):\n",
    "        if is_xsrc and pd.isnull(is_ysrc): return 'x'\n",
    "        if is_ysrc and pd.isnull(is_xsrc): return 'y'\n",
    "        else: return None\n",
    "\n",
    "    outcomes_df['is_x_or_y'] = np.vectorize(is_x_or_y)(outcomes_df['is_xsrc'], outcomes_df['is_ysrc'])\n",
    "    outcomes_df['is_single_src'] = outcomes_df['is_single_xsrc'] | outcomes_df['is_single_ysrc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting outcomes\n",
      "Joining feature and outcome DFs\n"
     ]
    }
   ],
   "source": [
    "outcomes_df_subset = format_outcomes_df(\n",
    "    outcomes_df,\n",
    "    config['outcome_variable_name'],\n",
    "    prediction_task_to_outcomes[config['outcome_variable_name']][config['prediction_task']],\n",
    "    id_field=config['id_field']\n",
    ")\n",
    "final_df = join_features_and_outcomes(features_df, outcomes_df_subset, on=config['id_field'])\n",
    "final_df.drop([config['id_field']], axis=1, inplace=True, errors='ignore')\n",
    "last_index = final_df.columns.get_loc(config['outcome_variable_name'])\n",
    "\n",
    "X = final_df.iloc[:, :last_index]\n",
    "y = final_df.iloc[:, last_index]\n",
    "\n",
    "subset = 100000\n",
    "X_subset, y_subset = resample(X, y, n_samples=subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Persisted Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_by_type = pickle.load(open(join(config['features_directory'], config['feature_set_lookup_file_name']), 'rb'))\n",
    "\n",
    "if dataset == 'dataset':\n",
    "    feature_names = feature_names_by_type['aggregate_single_field'] + feature_names_by_type['aggregate_pairwise_field']\n",
    "\n",
    "    dimensions_feature_names = ['exists-agg', 'length-agg']\n",
    "    type_feature_names = ['data_type', 'general_type']\n",
    "    value_feature_names = [ 'min-agg', 'max-agg', 'mean-agg', 'median-agg', 'range-agg', 'var-agg', 'std-agg', 'range_overlap', 'is_normal', 'q25-agg', 'q75-agg', 'kurtosis-agg', '_none', 'unique', 'mode', 'anova', 'nested', 'chi_sq', 'ks_', 'correlation', 'shared_elements', 'identical', 'sequence', '_space-agg', 'sorted', 'entropy-agg-', 'gini-agg-', 'abs_dev-agg-', 'normality', 'monotonic', 'outliers', 'moment', 'quant_coeff_disp', 'skewness', 'value_length']\n",
    "    name_feature_names = ['in_name', 'edit_distance-', 'uppercase', 'shared_words', 'name_length']\n",
    "\n",
    "    unique_feature_sets = {\n",
    "        'basic': [],  # feature_names_by_type['basic'],\n",
    "        'dimensions': [ x for x in feature_names if any(x.startswith(e) for e in dimensions_feature_names) ],\n",
    "        'types': [ x for x in feature_names if any(e in x for e in type_feature_names) ],\n",
    "        'values': [ x for x in feature_names if any(e in x for e in value_feature_names) ],\n",
    "        'names': [ x for x in feature_names if any(e in x for e in name_feature_names) ]\n",
    "    }\n",
    "    \n",
    "    feature_sets = {\n",
    "        'dimensions': unique_feature_sets['basic'] + unique_feature_sets['dimensions'],\n",
    "        'types': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'],\n",
    "        'values': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'] + unique_feature_sets['values'],\n",
    "        'names': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'] + unique_feature_sets['values'] + unique_feature_sets['names']\n",
    "    }\n",
    "if dataset == 'field':\n",
    "    feature_names = feature_names_by_type['single_field']\n",
    "    dimensions_feature_names = ['length']\n",
    "    type_feature_names = ['data_type', 'general_type']\n",
    "    value_feature_names = ['has_none', 'percentage_none', 'num_none', 'num_unique_elements', 'unique_percent', 'is_unique', 'list_entropy', 'mean_value_length', 'median_value_length', 'min_value_length', 'max_value_length', 'std_value_length', 'percentage_of_mode', 'mean', 'normalized_mean', 'median', 'normalized_median', 'var', 'std', 'coeff_var', 'min', 'max', 'range', 'normalized_range', 'entropy', 'gini', 'q25', 'q75', 'med_abs_dev', 'avg_abs_dev', 'quant_coeff_disp', 'skewness', 'kurtosis', 'moment_5', 'moment_6', 'moment_7', 'moment_8', 'moment_9', 'moment_10', 'percent_outliers_15iqr', 'percent_outliers_3iqr', 'percent_outliers_1_99', 'percent_outliers_3std', 'has_outliers_15iqr', 'has_outliers_3iqr', 'has_outliers_1_99', 'has_outliers_3std', 'normality_statistic', 'normality_p', 'is_normal_5', 'is_normal_1', 'is_sorted', 'is_monotonic', 'sortedness', 'lin_space_sequence_coeff', 'log_space_sequence_coeff', 'is_lin_space', 'is_log_space']\n",
    "    name_feature_names = ['in_name', 'edit_distance-', 'uppercase', 'shared_words', 'name_length']\n",
    "\n",
    "    unique_feature_sets = {\n",
    "        'basic': [],  # feature_names_by_type['basic'],\n",
    "        'dimensions': dimensions_feature_names,\n",
    "        'types': [ x for x in feature_names if any(e in x for e in type_feature_names) ],\n",
    "        'values': value_feature_names,\n",
    "        'names': [ x for x in feature_names if any(e in x for e in name_feature_names) ]\n",
    "    }\n",
    "    \n",
    "    feature_sets = {\n",
    "        'dimensions': unique_feature_sets['basic'] + unique_feature_sets['dimensions'],\n",
    "        'types': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'],\n",
    "        'values': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'] + unique_feature_sets['values'],\n",
    "        'names': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'] + unique_feature_sets['values'] + unique_feature_sets['names']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = feature_names_by_type['single_field']\n",
    "dimensions_feature_names = ['length']\n",
    "type_feature_names = ['data_type', 'general_type']\n",
    "value_feature_names = ['has_none', 'percentage_none', 'num_none', 'num_unique_elements', 'unique_percent', 'is_unique', 'list_entropy', 'mean_value_length', 'median_value_length', 'min_value_length', 'max_value_length', 'std_value_length', 'percentage_of_mode', 'mean', 'normalized_mean', 'median', 'normalized_median', 'var', 'std', 'coeff_var', 'min', 'max', 'range', 'normalized_range', 'entropy', 'gini', 'q25', 'q75', 'med_abs_dev', 'avg_abs_dev', 'quant_coeff_disp', 'skewness', 'kurtosis', 'moment_5', 'moment_6', 'moment_7', 'moment_8', 'moment_9', 'moment_10', 'percent_outliers_15iqr', 'percent_outliers_3iqr', 'percent_outliers_1_99', 'percent_outliers_3std', 'has_outliers_15iqr', 'has_outliers_3iqr', 'has_outliers_1_99', 'has_outliers_3std', 'normality_statistic', 'normality_p', 'is_normal_5', 'is_normal_1', 'is_sorted', 'is_monotonic', 'sortedness', 'lin_space_sequence_coeff', 'log_space_sequence_coeff', 'is_lin_space', 'is_log_space']\n",
    "name_feature_names = ['in_name', 'edit_distance-', 'uppercase', 'shared_words', 'name_length']\n",
    "\n",
    "unique_feature_sets = {\n",
    "    'basic': [],  # feature_names_by_type['basic'],\n",
    "    'dimensions': dimensions_feature_names,\n",
    "    'types': [ x for x in feature_names if any(e in x for e in type_feature_names) ],\n",
    "    'values': value_feature_names,\n",
    "    'names': [ x for x in feature_names if any(e in x for e in name_feature_names) ]\n",
    "}\n",
    "\n",
    "feature_sets = {\n",
    "    'dimensions': unique_feature_sets['basic'] + unique_feature_sets['dimensions'],\n",
    "    'types': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'],\n",
    "    'values': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'] + unique_feature_sets['values'],\n",
    "    'names': unique_feature_sets['basic'] + unique_feature_sets['dimensions'] + unique_feature_sets['types'] + unique_feature_sets['values'] + unique_feature_sets['names']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_feature_sets['values']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load(join(base_results_dir, '', model_file_name))\n",
    "feature_set_names = [ c for c in feature_sets['names']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = feature_set_names\n",
    "if model == 'lr':\n",
    "    importances = np.abs(clf.coef_[0])\n",
    "if model == 'rf':\n",
    "    columns = feature_set_names\n",
    "    importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_importance_table = [ [columns[i], importances[i]] for i in indices ][:]\n",
    "feature_importance_df = pd.DataFrame(feature_importance_table, columns=['feature', 'importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df['aggregated'] = feature_importance_df['feature'].apply(lambda k: k.split('-agg-')[0])\n",
    "feature_importance_df.groupby('aggregated', as_index=False).aggregate(['max', 'mean', 'sum']).sort_values([('importance', 'max')], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
